# data_loader.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import logging
import os
import joblib  # –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫–∞–ª–µ—Ä–∞

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_and_process_data():
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Wine Quality
    """
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {url}")

    df = pd.read_csv(url, delimiter=';')
    logger.info(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –§–æ—Ä–º–∞: {df.shape}")

    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        df = df.drop_duplicates()
        logger.info(f"–î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {df.shape}")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    null_count = df.isnull().sum().sum()
    logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {null_count}")

    # 4. Feature Engineering - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –î–û —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è!
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    df = create_new_features(df)

    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    target_column = 'quality'
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test –î–û –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ data leakage)
    logger.info("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 7. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –û–¢–î–ï–õ–¨–ù–û –¥–ª—è train –∏ test
    logger.info("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)

    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    save_artifacts(X_train_scaled, X_test_scaled, y_train, y_test, scaler)

    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler


def create_new_features(df):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    """
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–µ–Ω–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –æ –≤–∏–Ω–µ
    df['acidity_ratio'] = df['fixed acidity'] / (df['volatile acidity'] + 1e-6)
    df['sulfur_ratio'] = df['free sulfur dioxide'] / (df['total sulfur dioxide'] + 1e-6)
    df['alcohol_acidity_interaction'] = df['alcohol'] * df['fixed acidity']
    df['density_pH_interaction'] = df['density'] * df['pH']

    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ skewed features
    skewed_features = ['residual sugar', 'chlorides', 'sulphates']
    for feature in skewed_features:
        df[f'log_{feature}'] = np.log1p(df[feature])

    # –ó–∞–º–µ–Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.fillna(df.median())

    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(skewed_features) + 4} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ù–æ–≤–∞—è —Ñ–æ—Ä–º–∞: {df.shape}")
    return df


def scale_data(X_train, X_test):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è train –∏ test
    """
    scaler = StandardScaler()

    # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)  # –ø—Ä–∏–º–µ–Ω—è–µ–º transform –∫ test!

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ DataFrame —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

    return X_train_scaled, X_test_scaled, scaler


def save_artifacts(X_train, X_test, y_train, y_test, scaler):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    """
    output_dir = 'data/processed'
    models_dir = 'models'

    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(models_dir, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train.to_csv(f'{output_dir}/X_train.csv', index=False)
        X_test.to_csv(f'{output_dir}/X_test.csv', index=False)
        y_train.to_csv(f'{output_dir}/y_train.csv', index=False)
        y_test.to_csv(f'{output_dir}/y_test.csv', index=False)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        joblib.dump(scaler, f'{models_dir}/scaler.joblib')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        feature_info = {
            'feature_names': X_train.columns.tolist(),
            'target_name': 'quality',
            'data_shape': {
                'X_train': X_train.shape,
                'X_test': X_test.shape,
                'y_train': y_train.shape,
                'y_test': y_test.shape
            }
        }
        joblib.dump(feature_info, f'{output_dir}/feature_info.joblib')

        logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
        logger.info(f"Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {models_dir}/scaler.joblib")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        raise


def get_data_info():
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        feature_info = joblib.load('data/processed/feature_info.joblib')
        logger.info("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {len(feature_info['feature_names'])}")
        logger.info(f"–†–∞–∑–º–µ—Ä—ã: {feature_info['data_shape']}")
        return feature_info
    except:
        logger.warning("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
    X_train, X_test, y_train, y_test, scaler = load_and_process_data()

    # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìä Train data: {X_train.shape}")
    print(f"üìä Test data: {X_test.shape}")
    print(f"üéØ Target distribution:")
    print(y_train.value_counts().sort_index())

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    print(f"\nüÜï –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    new_features = [col for col in X_train.columns if any(x in col for x in ['ratio', 'interaction', 'log_'])]
    for feature in new_features:
        print(f"  - {feature}")